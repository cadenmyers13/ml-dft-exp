{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Concatenate, Dense, Dropout, Lambda, BatchNormalization, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in dataframe\n",
    "This df contains atomic features, soap descriptors, and bandgap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/data/soap_and_atomic_features.pkl.gz\"\n",
    "# Step 1: Read the compressed pickle\n",
    "with gzip.open(data_path, 'rb') as f:\n",
    "    data_df = pickle.load(f)\n",
    "\n",
    "print(data_df.shape)\n",
    "# data_df = data_df[data_df['gap opt'] >= 0.2]\n",
    "data_df = data_df.dropna()\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Define the features to plot\n",
    "features = [\n",
    "    'electronegativity_mean', 'electronegativity_max', 'electronegativity_min', 'electronegativity_std',\n",
    "    'atomic_radius_mean', 'atomic_radius_max', 'atomic_radius_min', 'atomic_radius_std',\n",
    "    'ionenergies_mean', 'ionenergies_max', 'ionenergies_min', 'ionenergies_std',\n",
    "    'covalent_radius_mean', 'covalent_radius_max', 'covalent_radius_min', 'covalent_radius_std',\n",
    "    'nvalence_mean', 'nvalence_max', 'nvalence_min', 'nvalence_std'\n",
    "]\n",
    "\n",
    "# Set up the grid\n",
    "fig, axes = plt.subplots(4, 5, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    sns.histplot(data_df[feature], ax=axes[i], kde=True, bins=100, color='steelblue')\n",
    "    axes[i].set_title(feature, fontsize=10)\n",
    "    axes[i].tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_gap = data_df['gap opt'] > 1\n",
    "plt.hist(has_gap.astype(int), bins=2, edgecolor='black')\n",
    "plt.xticks([0, 1], ['No Gap', 'Has Gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_atomic_8 = data_df[[\n",
    "    'electronegativity_mean', 'electronegativity_std', \n",
    "    'atomic_radius_mean', 'atomic_radius_std',\n",
    "    'ionenergies_mean', 'ionenergies_std', \n",
    "    'covalent_radius_mean', 'covalent_radius_std',\n",
    "]]\n",
    "soaps = np.array(data_df['padded_soap'].tolist())\n",
    "X_soap_2d = soaps[..., np.newaxis]  # add channel dim: (N, 64, 800, 1)\n",
    "X_atomic_8 = X_atomic_8.to_numpy()\n",
    "print('X_soap_2d shape:', X_soap_2d.shape)\n",
    "print('X_atomic_8 shape:', X_atomic_8.shape)\n",
    "print('--------------------')\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_soap_train, X_soap_test, X_atomic_8_train, X_atomic_8_test, y_train, y_test = train_test_split(\n",
    "    X_soap_2d, X_atomic_8, data_df['gap opt'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "bg_threshold = 0.02 # eV\n",
    "y_train_binary = (y_train > bg_threshold).astype(int)\n",
    "y_test_binary = (y_test > bg_threshold).astype(int)\n",
    "\n",
    "# Flatten the soap descriptors for scaling\n",
    "X_soap_train_flat = X_soap_train.reshape(X_soap_train.shape[0], -1)\n",
    "X_soap_test_flat = X_soap_test.reshape(X_soap_test.shape[0], -1)\n",
    "\n",
    "# scale soap descriptors\n",
    "scaler_soap = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training set, and transform the test set\n",
    "X_soap_train_scaled = scaler_soap.fit_transform(X_soap_train_flat)\n",
    "X_soap_test_scaled = scaler_soap.transform(X_soap_test_flat)\n",
    "\n",
    "# Reshape back to the original shape (N, 64, 800, 1)\n",
    "X_soap_train_scaled = X_soap_train_scaled.reshape(X_soap_train.shape)\n",
    "X_soap_test_scaled = X_soap_test_scaled.reshape(X_soap_test.shape)\n",
    "\n",
    "# scale atomic input data\n",
    "scaler_atomic_8 = MinMaxScaler()\n",
    "\n",
    "X_atomic_8_train_scaled = scaler_atomic_8.fit_transform(X_atomic_8_train)\n",
    "X_atomic_8_test_scaled = scaler_atomic_8.transform(X_atomic_8_test)\n",
    "\n",
    "print('X_soap_train_scaled shape:', X_soap_train_scaled.shape)\n",
    "print('X_soap_test_scaled shape:', X_soap_test_scaled.shape)\n",
    "print('X_atomic_8_train_scaled shape:', X_atomic_8_train_scaled.shape)\n",
    "print('X_atomic_8_test_scaled shape:', X_atomic_8_test_scaled.shape)\n",
    "print('--------------------')\n",
    "print('y_train_binary shape:', y_train_binary.shape)\n",
    "print('y_test_binary shape:', y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_atomic_20 = data_df.drop(columns=['formula', 'mpid', 'gap opt', 'padded_soap', 'soap_flat']).to_numpy()\n",
    "print('X_soap_2d shape:', X_soap_2d.shape)\n",
    "print('X_atomic_20 shape:', X_atomic_20.shape)\n",
    "print('--------------------')\n",
    "# Step 2: Split the data into training and testing sets\n",
    "_, _, X_atomic_20_train, X_atomic_20_test, y_train, y_test = train_test_split(\n",
    "    X_soap_2d, X_atomic_20, data_df['gap opt'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale atomic input data\n",
    "scaler_atomic_20 = MinMaxScaler()\n",
    "\n",
    "X_atomic_20_train_scaled = scaler_atomic_20.fit_transform(X_atomic_20_train)\n",
    "X_atomic_20_test_scaled = scaler_atomic_20.transform(X_atomic_20_test)\n",
    "\n",
    "print('X_soap_train_scaled shape:', X_soap_train_scaled.shape)\n",
    "print('X_soap_test_scaled shape:', X_soap_test_scaled.shape)\n",
    "print('---------------------')\n",
    "print('X_atomic_20_train_scaled shape:', X_atomic_20_train_scaled.shape)\n",
    "print('X_atomic_20_test_scaled shape:', X_atomic_20_test_scaled.shape)\n",
    "print('X_atomic_8_train_scaled shape:', X_atomic_8_train_scaled.shape)\n",
    "print('X_atomic_8_test_scaled shape:', X_atomic_8_test_scaled.shape)\n",
    "print('--------------------')\n",
    "print('y_train_binary shape:', y_train_binary.shape)\n",
    "print('y_test_binary shape:', y_test_binary.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOAP input branch\n",
    "soap_input = Input(shape=(64, 800, 1), name='soap_input')\n",
    "x = Conv2D(32, (3, 3), activation='relu')(soap_input)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Periodic table input (8 features)\n",
    "pt_input = Input(shape=(8,), name='periodic_features')\n",
    "\n",
    "# Feature group slices (each group has 2 features)\n",
    "electronegativity = Lambda(lambda t: t[:, 0:2])(pt_input)\n",
    "atomic_radius     = Lambda(lambda t: t[:, 2:4])(pt_input)\n",
    "ion_energies      = Lambda(lambda t: t[:, 4:6])(pt_input)\n",
    "covalent_radius   = Lambda(lambda t: t[:, 6:8])(pt_input)\n",
    "\n",
    "# Process each group with a small Dense layer\n",
    "e_dense = Dense(8, activation='relu')(electronegativity)\n",
    "r_dense = Dense(8, activation='relu')(atomic_radius)\n",
    "i_dense = Dense(8, activation='relu')(ion_energies)\n",
    "c_dense = Dense(8, activation='relu')(covalent_radius)\n",
    "\n",
    "# Concatenate all group representations\n",
    "y = Concatenate()([e_dense, r_dense, i_dense, c_dense])\n",
    "y = Dense(64, activation='relu')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Dense(32, activation='relu')(y)\n",
    "\n",
    "# Merge with SOAP branch\n",
    "combined = Concatenate()([x, y])\n",
    "z = Dense(128, activation='relu')(combined)\n",
    "z = Dropout(0.3)(z)\n",
    "z = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=[soap_input, pt_input], outputs=z)\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall(), AUC()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compute class weights from labels\n",
    "y_train_array = np.array(y_train_binary)\n",
    "\n",
    "# Correctly format the `classes` parameter\n",
    "classes = np.unique(y_train_array)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        min_delta=0.001,  # Minimum accuracy improvement required\n",
    "        patience=5,       # Number of epochs with no improvement to wait\n",
    "        verbose=1,        # Display messages when stopping\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    {'soap_input': X_soap_train, 'periodic_features': X_atomic_train},\n",
    "    y_train_binary,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/models/bandgap_classifier_pt_slicing_std_avg.h5')\n",
    "model.save('/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/models/bandgap_classifier_pt_slicing_std_avg.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(\n",
    "    {'soap_input': X_soap_test, 'periodic_features': X_atomic_test},\n",
    "    y_test_binary,  # Use binary labels\n",
    "    verbose=0\n",
    ")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}, Precision: {test_precision:.3f}, Recall: {test_recall:.3f}, AUC: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(\n",
    "    {'soap_input': X_soap_test, 'periodic_features': X_atomic_test},\n",
    "    y_test_binary,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation metrics:\", dict(zip(model.metrics_names, results)))\n",
    "\n",
    "# prediction\n",
    "y_pred_probs = model.predict({'soap_input': X_soap_test, 'periodic_features': X_atomic_test})\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred, target_names=['No Gap', 'Has Gap']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "plt.imshow(confusion_matrix(y_test_binary, y_pred), cmap='RdBu', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# print(confusion_matrix(y_test_binary, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best performing model based on f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "soap_input = Input(shape=(64, 800, 1), name='soap_input')\n",
    "periodic_features = Input(shape=(8,), name='periodic_features')\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# CNN on SOAP input\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(soap_input)              # (62, 798, 32)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)                                          # (31, 399, 32)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)                       # (29, 397, 64)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)                                          # (14, 198, 64)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)                                               # (64,)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Dense on periodic features\n",
    "y = Dense(64, activation='relu')(periodic_features)\n",
    "y = BatchNormalization()(y)                                                    # (64,)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Combine\n",
    "combined = Concatenate()([x, y])                                               # (128,)\n",
    "combined = Dense(128, activation='relu')(combined)\n",
    "combined = Dropout(0.5)(combined)  # adjust if needed\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Build model\n",
    "model = Model(inputs=[soap_input, periodic_features], outputs=output)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# (Optional) Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - accuracy: 0.7632 - loss: 0.5855 - val_accuracy: 0.8285 - val_loss: 0.4291\n",
      "Epoch 2/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - accuracy: 0.7972 - loss: 0.4695 - val_accuracy: 0.8448 - val_loss: 0.3748\n",
      "Epoch 3/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1s/step - accuracy: 0.8063 - loss: 0.4460 - val_accuracy: 0.8495 - val_loss: 0.3804\n",
      "Epoch 4/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.8262 - loss: 0.4354 - val_accuracy: 0.8156 - val_loss: 0.4441\n",
      "Epoch 5/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.8234 - loss: 0.4433 - val_accuracy: 0.7468 - val_loss: 0.5335\n",
      "Epoch 6/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8218 - loss: 0.4393 - val_accuracy: 0.8541 - val_loss: 0.3475\n",
      "Epoch 7/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.8245 - loss: 0.4108 - val_accuracy: 0.8483 - val_loss: 0.3460\n",
      "Epoch 8/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8321 - loss: 0.4050 - val_accuracy: 0.7561 - val_loss: 0.5472\n",
      "Epoch 9/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.8207 - loss: 0.4084 - val_accuracy: 0.8565 - val_loss: 0.3544\n",
      "Epoch 10/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.8199 - loss: 0.4227 - val_accuracy: 0.8448 - val_loss: 0.3647\n",
      "Epoch 11/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8311 - loss: 0.4172 - val_accuracy: 0.7771 - val_loss: 0.5255\n",
      "Epoch 12/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 988ms/step - accuracy: 0.8389 - loss: 0.3859 - val_accuracy: 0.8471 - val_loss: 0.3535\n",
      "Epoch 13/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 992ms/step - accuracy: 0.8414 - loss: 0.3872 - val_accuracy: 0.8191 - val_loss: 0.3969\n",
      "Epoch 14/30\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 989ms/step - accuracy: 0.8402 - loss: 0.3761 - val_accuracy: 0.8413 - val_loss: 0.3639\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        min_delta=0.001,  # Minimum accuracy improvement required\n",
    "        patience=5,       # Number of epochs with no improvement to wait\n",
    "        verbose=1,        # Display messages when stopping\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    {'soap_input': X_soap_train, 'periodic_features': X_atomic_8_train},\n",
    "    y_train_binary,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 - 8s - 235ms/step - accuracy: 0.8422 - loss: 0.3711\n",
      "\n",
      "Evaluation metrics: {'loss': 0.3710688650608063, 'compile_metrics': 0.8422035574913025}\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No Gap       0.71      0.74      0.73       301\n",
      "     Has Gap       0.90      0.88      0.89       770\n",
      "\n",
      "    accuracy                           0.84      1071\n",
      "   macro avg       0.80      0.81      0.81      1071\n",
      "weighted avg       0.84      0.84      0.84      1071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(\n",
    "    {'soap_input': X_soap_test, 'periodic_features': X_atomic_8_test},\n",
    "    y_test_binary,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation metrics:\", dict(zip(model.metrics_names, results)))\n",
    "\n",
    "# prediction\n",
    "y_pred_probs = model.predict({'soap_input': X_soap_test, 'periodic_features': X_atomic_8_test})\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred, target_names=['No Gap', 'Has Gap']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/models/bandgap_classifier_pt_no_slicing_std_avg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net only using atomic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periodic table feature columns\n",
    "pt_columns = [\n",
    "    'electronegativity_mean', 'electronegativity_max', 'electronegativity_min', 'electronegativity_std',\n",
    "    'atomic_radius_mean', 'atomic_radius_max', 'atomic_radius_min', 'atomic_radius_std',\n",
    "    'ionenergies_mean', 'ionenergies_max', 'ionenergies_min', 'ionenergies_std',\n",
    "    'covalent_radius_mean', 'covalent_radius_max', 'covalent_radius_min', 'covalent_radius_std',\n",
    "    'nvalence_mean', 'nvalence_max', 'nvalence_min', 'nvalence_std'\n",
    "]\n",
    "\n",
    "# Extract features and labels\n",
    "X = data_df[pt_columns].values\n",
    "y = data_df['gap opt'].values\n",
    "\n",
    "print('y shape:', y.shape)\n",
    "print('X shape:', X.shape)\n",
    "print('----------------')\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print('X_train_scaled shape:', X_train_scaled.shape)\n",
    "print('X_test_scaled shape:', X_test_scaled.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "classes = np.unique(y_train)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build NN model\n",
    "pt_input = Input(shape=(X_train_scaled.shape[1],), name='periodic_features')\n",
    "x = Dense(64, activation='relu')(pt_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=pt_input, outputs=output)\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', Precision(), Recall(), AUC()])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(X_train_scaled, y_train,\n",
    "          validation_data=(X_test_scaled, y_test),\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          class_weight=class_weights_dict,\n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "results = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(verbosity=2)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/data/xgb_model_all_data_oob.pkl\"\n",
    "# with open(model_path, \"wb\") as model_file:\n",
    "#     pickle.dump(xgb, model_file)\n",
    "# print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(X_test)\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "print(f\"XGBoost MAE: {xgb_mae:.4f}\")\n",
    "# Plotting the predictions\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y_test, xgb_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('True bandgap')\n",
    "plt.ylabel('predicted bandgap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict({'soap_input': X_soap_test, 'periodic_features': X_atomic_test})\n",
    "\n",
    "# Parity plot for predicted vs true bandgap values (Neural Network)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, label='Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Ideal')\n",
    "plt.xlabel('True Bandgap (eV)')\n",
    "plt.ylabel('Predicted Bandgap (eV)')\n",
    "plt.title('Parity Plot (Neural Network)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
