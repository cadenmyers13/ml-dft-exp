{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "\n",
    "This notebook is designed to read in and evaluate the performance of our model. Everything should run (hopefully) if you follow these instructions:\n",
    "\n",
    "1) Download the models from the shared Google Drive (I've uploaded them) and make sure you have the data saved as well (you should have this from last time).\n",
    "2) Make sure the model files are located in the right place, I recommend having it under `ml4ms_bandgap_final/models/`.\n",
    "3) Get and configure the correct data and model path. This can be done by right-clicking on your file/directory and selecting \"copy path\". Or this can be done by opening a terminal (any terminal will do) and `cd` into the `ml4ms_bandgap_final` directory. then `cd` into where ever you have the data `cd data` and type `pwd`. This should print the path to your data. Following the same logic, you can do the same to get the path to your models (`cd models` etc...)\n",
    "4) Once you have those paths, you'll need to update the corresponding paths below. This may take some effort but read the error messages. They will tell you where things are failing.\n",
    "5) If everything works okay, you should be able to hit \"Run All\" at the top, and evaluation metrics will be output for each model.\n",
    "\n",
    "Look at the outputs and try to understand them. This info is important and tells us how our model performed. This info will be in the report too. If you are unsure what some of the metrics mean (f1 score, recall, etc) look it up, they typically are pretty simple! Hopefully this help! lmk if you have questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_absolute_error, classification_report\n",
    "\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, MeanAbsoluteError\n",
    "import gzip\n",
    "import pickle\n",
    "import absl.logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/data/soap_and_atomic_features.pkl.gz\"\n",
    "# Step 1: Read the compressed pickle\n",
    "with gzip.open(data_path, 'rb') as f:\n",
    "    data_df = pickle.load(f)\n",
    "\n",
    "data_df = data_df.dropna()\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_atomic_8 = data_df[[\n",
    "    'electronegativity_mean', 'electronegativity_std', \n",
    "    'atomic_radius_mean', 'atomic_radius_std',\n",
    "    'ionenergies_mean', 'ionenergies_std', \n",
    "    'covalent_radius_mean', 'covalent_radius_std',\n",
    "]]\n",
    "soaps = np.array(data_df['padded_soap'].tolist())\n",
    "X_soap_2d = soaps[..., np.newaxis]  # add channel dim: (N, 64, 800, 1)\n",
    "X_atomic_8 = X_atomic_8.to_numpy()\n",
    "print('X_soap_2d shape:', X_soap_2d.shape)\n",
    "print('X_atomic_8 shape:', X_atomic_8.shape)\n",
    "print('--------------------')\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_soap_train, X_soap_test, X_atomic_8_train, X_atomic_8_test, y_train, y_test = train_test_split(\n",
    "    X_soap_2d, X_atomic_8, data_df['gap opt'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "bg_threshold = 0.02 # eV\n",
    "y_train_binary = (y_train > bg_threshold).astype(int)\n",
    "y_test_binary = (y_test > bg_threshold).astype(int)\n",
    "\n",
    "# Flatten the soap descriptors for scaling\n",
    "X_soap_train_flat = X_soap_train.reshape(X_soap_train.shape[0], -1)\n",
    "X_soap_test_flat = X_soap_test.reshape(X_soap_test.shape[0], -1)\n",
    "\n",
    "# scale soap descriptors\n",
    "scaler_soap = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training set, and transform the test set\n",
    "X_soap_train_scaled = scaler_soap.fit_transform(X_soap_train_flat)\n",
    "X_soap_test_scaled = scaler_soap.transform(X_soap_test_flat)\n",
    "\n",
    "# Reshape back to the original shape (N, 64, 800, 1)\n",
    "X_soap_train_scaled = X_soap_train_scaled.reshape(X_soap_train.shape)\n",
    "X_soap_test_scaled = X_soap_test_scaled.reshape(X_soap_test.shape)\n",
    "\n",
    "# scale atomic input data\n",
    "scaler_atomic_8 = MinMaxScaler()\n",
    "\n",
    "X_atomic_8_train_scaled = scaler_atomic_8.fit_transform(X_atomic_8_train)\n",
    "X_atomic_8_test_scaled = scaler_atomic_8.transform(X_atomic_8_test)\n",
    "\n",
    "print('X_soap_train_scaled shape:', X_soap_train_scaled.shape)\n",
    "print('X_soap_test_scaled shape:', X_soap_test_scaled.shape)\n",
    "print('X_atomic_8_train_scaled shape:', X_atomic_8_train_scaled.shape)\n",
    "print('X_atomic_8_test_scaled shape:', X_atomic_8_test_scaled.shape)\n",
    "print('--------------------')\n",
    "print('y_train_binary shape:', y_train_binary.shape)\n",
    "print('y_test_binary shape:', y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_atomic_20 = data_df.drop(columns=['formula', 'mpid', 'gap opt', 'padded_soap', 'soap_flat']).to_numpy()\n",
    "print('X_soap_2d shape:', X_soap_2d.shape)\n",
    "print('X_atomic_20 shape:', X_atomic_20.shape)\n",
    "print('--------------------')\n",
    "# Step 2: Split the data into training and testing sets\n",
    "_, _, X_atomic_20_train, X_atomic_20_test, y_train, y_test = train_test_split(\n",
    "    X_soap_2d, X_atomic_20, data_df['gap opt'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale atomic input data\n",
    "scaler_atomic_20 = MinMaxScaler()\n",
    "\n",
    "X_atomic_20_train_scaled = scaler_atomic_20.fit_transform(X_atomic_20_train)\n",
    "X_atomic_20_test_scaled = scaler_atomic_20.transform(X_atomic_20_test)\n",
    "\n",
    "print('X_soap_train_scaled shape:', X_soap_train_scaled.shape)\n",
    "print('X_soap_test_scaled shape:', X_soap_test_scaled.shape)\n",
    "print('---------------------')\n",
    "print('X_atomic_20_train_scaled shape:', X_atomic_20_train_scaled.shape)\n",
    "print('X_atomic_20_test_scaled shape:', X_atomic_20_test_scaled.shape)\n",
    "print('X_atomic_8_train_scaled shape:', X_atomic_8_train_scaled.shape)\n",
    "print('X_atomic_8_test_scaled shape:', X_atomic_8_test_scaled.shape)\n",
    "print('--------------------')\n",
    "print('y_train_binary shape:', y_train_binary.shape)\n",
    "print('y_test_binary shape:', y_test_binary.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress absl warnings about compiled metrics\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "model_dir = '/Users/cadenmyers/billingelab/dev/ml4ms_bandgap_final/models/'\n",
    "model_files = [\n",
    "    'bandgap_classifier_pt_slicing_std_avg.h5',\n",
    "    'bandgap_classifier_pt_slicing.h5',\n",
    "    'bandgap_classifier.h5',\n",
    "    'bandgap_cnn_model.h5'\n",
    "]\n",
    "\n",
    "def evaluate_classifier_model(model_file,  X_atomic_test, X_soap_test=X_soap_test, y_test_binary=y_test_binary, model_dir=model_dir):\n",
    "    \"\"\"\n",
    "    Evaluate a classifier model and print metrics.\n",
    "    Args:\n",
    "        model_file (str): Path to the model file.\n",
    "        X_soap_test (np.ndarray): Test SOAP features.\n",
    "        X_atomic_test (np.ndarray): Test atomic features.\n",
    "        y_test_binary (np.ndarray): Binary test labels.\n",
    "        model_dir (str): Directory containing the model file.\n",
    "    \"\"\"\n",
    "\n",
    "    print('-' * 100)\n",
    "    print('Loading classifier model:', model_file)\n",
    "    print('-' * 100)\n",
    "\n",
    "    # Classification-specific metrics\n",
    "    custom_objects = {\n",
    "        'auc_5': AUC(name='auc_5'),\n",
    "        'precision_5': Precision(name='precision_5'),\n",
    "        'recall_5': Recall(name='recall_5'),\n",
    "    }\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_dir + model_file, custom_objects=custom_objects)\n",
    "\n",
    "    # Evaluate\n",
    "    results = model.evaluate(\n",
    "        {'soap_input': X_soap_test, 'periodic_features': X_atomic_test},\n",
    "        y_test_binary,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(f\"\\nEvaluation Metrics for {model_file}:\")\n",
    "    print(dict(zip(model.metrics_names, results)))\n",
    "\n",
    "    # Predict and classify\n",
    "    y_pred_probs = model.predict({'soap_input': X_soap_test, 'periodic_features': X_atomic_test}, verbose=0)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    print(f\"\\nClassification Report: {model_file}\")\n",
    "    print(classification_report(y_test_binary, y_pred, target_names=[\"No Gap\", \"Has Gap\"]))\n",
    "\n",
    "\n",
    "def evaluate_regressor_model(model_file, X_soap_test, y_test=y_test, model_dir=model_dir):\n",
    "    \"\"\"\n",
    "    Evaluate a regression model and print metrics.\n",
    "    Args:\n",
    "        model_file (str): Path to the model file.\n",
    "        X_soap_test (np.ndarray): Test SOAP features.\n",
    "        y_test (np.ndarray): Test labels.\n",
    "        model_dir (str): Directory containing the model file.\n",
    "    \"\"\"\n",
    "    print('-' * 100)\n",
    "    print('Loading regression model:', model_file)\n",
    "    print('-' * 100)\n",
    "    # Custom objects (for handling 'mae' string in saved model)\n",
    "    custom_objects = {\n",
    "        'mae': MeanAbsoluteError()\n",
    "    }\n",
    "    # Load model\n",
    "    model = load_model(model_dir + model_file, custom_objects=custom_objects)\n",
    "\n",
    "    # Evaluate\n",
    "    results = model.evaluate(X_soap_test, y_test, verbose=0)\n",
    "    print(f\"\\nEvaluation Metrics for {model_file}:\")\n",
    "    print(dict(zip(model.metrics_names, results)))\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_soap_test, verbose=0)\n",
    "\n",
    "    # Prediction summary\n",
    "    print(f\"\\nPrediction Summary for {model_file}:\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f} eV\")\n",
    "    return y_pred, y_test\n",
    "\n",
    "\n",
    "def evaluate_rf_model(rf_file, X_soap_test_flat, y_test=y_test, model_dir=model_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the Random Forest model on the test set.\n",
    "    Args:\n",
    "        rf_file (str): Path to the Random Forest model file.\n",
    "        X_soap_test_flat (np.ndarray): Flattened test SOAP features.\n",
    "        y_test (np.ndarray): Test labels.\n",
    "        model_dir (str): Directory containing the model file.\n",
    "    \"\"\"\n",
    "    print('-' * 100)\n",
    "    print('Loading Random Forest model:', rf_file)\n",
    "    print('-' * 100)\n",
    "\n",
    "\n",
    "    rf_model = joblib.load(model_dir + rf_file)\n",
    "    y_pred = rf_model.predict(X_soap_test_flat)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Random Forest MAE: {mae:.4f} eV\")\n",
    "    return y_pred, y_test, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: std and avg atomic values\n",
    "**Neural Net**: loads and displays metrics for `bandgap_classifier_pt_slicing_std_avg.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier_model(model_files[0], X_atomic_8_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: std, avg, max, and min atomic values\n",
    "**Neural Net**: loads and displays metrics for `bandgap_classifier_pt_slicing.h5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier_model(model_files[1], X_atomic_20_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: std, avg, min and max atomic values, no slicing\n",
    "**Neural Net**: loads and displays metrics for `bandgap_classifier.h5`. This model treats the atomic values the same as the SOAP values.\n",
    "\n",
    "## **Double check this by reruning this model!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier_model(model_files[2], X_atomic_20_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor: only SOAP descriptors\n",
    "**Convolutional Neural Net**: loads and displays metrics for `bandgap_cnn_model.h5`. This model treats the atomic values the same as the SOAP values and tries to **predict actual band gap values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_test = evaluate_regressor_model(model_files[3], X_soap_test)\n",
    "\n",
    "\n",
    "# Plotting the predictions\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "# Plot parity line\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "plt.xlabel(\"True Band Gap (eV)\")\n",
    "plt.ylabel(\"Predicted Band Gap (eV)\")\n",
    "plt.title(\"Parity Plot: True vs Predicted Band Gap\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor: Random forest on SOAP data\n",
    "loads and displays metrics for `random_forest_model.pkl`. This model was trained on all data. The filtered model (`filtered_random_forest_model.pkl`) was only ran on SOAP descriptors where the bandgap was greater than 0.02 eV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_files = ['random_forest_model.pkl', 'filtered_random_forest_model.pkl']\n",
    "\n",
    "y_pred_rf, y_test_rf, mae_rf = evaluate_rf_model(rf_files[0], X_soap_test_flat)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test_rf, y_pred_rf, alpha=0.5)\n",
    "# Plot parity line\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "plt.xlabel(\"True Band Gap (eV)\")\n",
    "plt.ylabel(\"Predicted Band Gap (eV)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
